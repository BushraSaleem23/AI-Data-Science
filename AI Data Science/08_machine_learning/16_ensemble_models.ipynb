{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Method:-\n",
    "\n",
    "It is a machine learning technique that combines several base models in order to produce one optimal predictive model.\n",
    "\n",
    "\n",
    "### Types of Ensemble Algorithm\n",
    "\n",
    "1. Bagging(Bootstrap Aggregating)\n",
    "2. Boosting\n",
    "3. Stacking(Stacked Generalization) \n",
    "\n",
    "1. ### Bagging\n",
    "\n",
    "It reduces variance and helps to avoid overfitting.\n",
    "It involves creating multiple subsets of the original dataset with replacement(booststrapping),\n",
    ". training a model on each sub set\n",
    ". and then aggregating their predictions.\n",
    ". Its common example is Random Forest Algorithm. \n",
    "\n",
    "2. ### Boosting\n",
    "\n",
    "It focuses on turning weak learners into strong ones iteratively.\n",
    "\n",
    ". It adjusts the weight of an observation based on the last classification\n",
    "\n",
    ". If an observation was classified incorrectly,it tries to increase the weight of this observation and vice versa.\n",
    "\n",
    ". Example include AdaBoost, Gradient Boosting and XGBoost.\n",
    "\n",
    "\n",
    "3. ### Stacking(Stacked Generalization)\n",
    "\n",
    "Stacking involves training  new model to combine the predictions of several base models.\n",
    "\n",
    "It typically involves two levels of models-base-level models and a meta-model that makes the final prediction based on the base level models output.\n",
    "\n",
    "## Why to use Ensemble Algorithms\n",
    "\n",
    ". Accuracy\n",
    ". Stability\n",
    ". Reduced overfitting\n",
    "\n",
    "## Applications of Ensemble Algorithm\n",
    "\n",
    ". Finances\n",
    ". Healthcare\n",
    ". Marketing\n",
    ". e-commerce\n",
    "\n",
    "### Challenges and consideration\n",
    "\n",
    ". Complexity\n",
    ". hard interpretability\n",
    ". parameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Techniques\n",
    "\n",
    "### 1. Gradient Boosting :-\n",
    "\n",
    "It works by fiting new models to the residual errors of prior models./it uses the loss function by using gradient decent and may be applied to both regression and classification problems.\n",
    "e.g: XGBoost, LightGBM\n",
    "\n",
    "### 2. CatBoost(Categorical Boosting):\n",
    "It ais an open source machine learning lagorithm that can handle categorical data directly and is based on gradient boosting.\n",
    "\n",
    "### 3. Stochastic Gradient Boosting:\n",
    "It fits each new model with random subsets of the training data and random subsets of the features..This helps to avoid overfitting and may result in improved performance.\n",
    "\n",
    "### 4. LPBoost (Linear Programming Boosting):\n",
    "It minimizes the exponential loss function using linear programming.'it is capable of handling a wide range of loss functions and may be applied to both regression and clasification issues.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
